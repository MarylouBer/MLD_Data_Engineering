{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPT63/b900zjm4nHyRFG+a7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarylouBer/MLD_Data_Engineering/blob/main/ETL_and_Datapipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0JbF6BEtpVu"
      },
      "outputs": [],
      "source": [
        "from datetime import timedelta\n",
        "from airflow.models import DAG\n",
        "from airflow.operators.python import PythonOperator\n",
        "from airflow.utils.dates import days_ago\n",
        "import tarfile\n",
        "\n",
        "\n",
        "default_args = {\n",
        "    'owner': 'MLD',\n",
        "    'start_date': days_ago(0),\n",
        "    'email': ['mld@test.com'],\n",
        "    'retries': 1,\n",
        "    'retry_delay': timedelta(minutes=5),\n",
        "}\n",
        "\n",
        "\n",
        "dag = DAG(\n",
        "    'process_web_log',\n",
        "    default_args=default_args,\n",
        "    description='process_web_log_capstoneproject',\n",
        "    schedule_interval=timedelta(days=1),\n",
        ")\n",
        "\n",
        "def extract_data():\n",
        "    input_file = '/home/project/airflow/dags/capstone/accesslog.txt'\n",
        "    output_file = '/home/project/airflow/dags/capstone/extracted_data.txt'\n",
        "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
        "        for line in infile:\n",
        "            ip = line.split()[0]\n",
        "            outfile.write(ip + '\\n')\n",
        "\n",
        "def transform_data():\n",
        "    input_file = '/home/project/airflow/dags/capstone/extracted_data.txt'\n",
        "    output_file = '/home/project/airflow/dags/capstone/transformed_data.txt'\n",
        "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
        "        for line in infile:\n",
        "            if '198.46.149.143' not in line:\n",
        "                outfile.write(line)\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    source_file = '/home/project/airflow/dags/capstone/transformed_data.txt'\n",
        "    tar_file = '/home/project/airflow/dags/capstone/weblog.tar'\n",
        "\n",
        "    with tarfile.open(tar_file, 'w') as tar:\n",
        "        tar.add(source_file, arcname='transformed_data.txt')\n",
        "\n",
        "\n",
        "# Task 1\n",
        "execute_extract_data = PythonOperator(\n",
        "    task_id='extract_data',\n",
        "    python_callable=extract_data,\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "# Task 2\n",
        "execute_transform_data = PythonOperator(\n",
        "    task_id='transform_data',\n",
        "    python_callable=transform_data,\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "# Task 3\n",
        "execute_load_data = PythonOperator(\n",
        "    task_id='load_data',\n",
        "    python_callable=load_data,\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "# Task pipeline\n",
        "execute_extract_data >> execute_transform_data >> execute_load_data\n"
      ]
    }
  ]
}